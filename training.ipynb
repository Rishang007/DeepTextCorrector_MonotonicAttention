{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"text_corrector.ipynb","provenance":[{"file_id":"1RZTnfBVJ8dJ4dpyNktFDXIQSIJMffbp_","timestamp":1614076193890}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"code","metadata":{"id":"psychological-porcelain","executionInfo":{"status":"ok","timestamp":1615639331265,"user_tz":-330,"elapsed":1719,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["import pandas as pd\n","import numpy as np\n","import regex as re\n","import pickle"],"id":"psychological-porcelain","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"myt025PGXYcE","executionInfo":{"status":"ok","timestamp":1615639336531,"user_tz":-330,"elapsed":4603,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"4bdae282-2e17-4c07-c0a6-ce7d6478169e"},"source":["import tensorflow as tf\n","tf.__version__"],"id":"myt025PGXYcE","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.4.1'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_PPASrqXdz8","executionInfo":{"status":"ok","timestamp":1615639336534,"user_tz":-330,"elapsed":3460,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"a8612406-51bb-4bd9-b3f6-ad392c1f5ee2"},"source":["tf.config.list_physical_devices('GPU')"],"id":"5_PPASrqXdz8","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"878jsZTGaHKz","executionInfo":{"status":"ok","timestamp":1615639336537,"user_tz":-330,"elapsed":3010,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["physical_devices = tf.config.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"],"id":"878jsZTGaHKz","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"least-nirvana"},"source":["## Data Preprocessing"],"id":"least-nirvana"},{"cell_type":"code","metadata":{"id":"biological-binding"},"source":["data = open(\"/content/drive/MyDrive/Project_2/data/final_data_250k.txt\")\n","#data = open(\"C:/Users/rpris/Google Drive/Project_2/data/final_data.txt\")\n","\n","text=[]\n","for f in data:\n","    f = re.sub(r\"\\n\",\"\",f)                 #removing /n\n","\n","    if 'yer' in f.split():         #character hagrid from harry potter special language removed, his special english\n","        continue\n","\n","    # specific\n","    f = re.sub(r\"won't\", \"will not\", f)              #TAKEN from assignment-20 text classification\n","    f = re.sub(r\"can\\'t\", \"can not\", f)\n","    # general\n","    f = re.sub(r\"n\\'t\", \" not\", f)\n","    f = re.sub(r\"\\'re\", \" are\", f)\n","    f = re.sub(r\"\\'s\", \" is\", f)\n","    f = re.sub(r\"\\'d\", \" would\", f)\n","    f = re.sub(r\"\\'ll\", \" will\", f)\n","    f = re.sub(r\"\\'t\", \" not\", f)\n","    f = re.sub(r\"\\'ve\", \" have\", f)\n","    f = re.sub(r\"\\'m\", \" am\", f)\n","    \n","    f = re.sub(r\"[^A-Za-z0-9\\s]\",\"\", f )   #removing any special characters except ?\n","    f = f.lower()\n","    text.append(f)\n","\n","    #break\n","data.close()"],"id":"biological-binding","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"paperback-wireless"},"source":["## Perturbations"],"id":"paperback-wireless"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"related-museum","executionInfo":{"status":"ok","timestamp":1615610881548,"user_tz":-330,"elapsed":14268,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"d08ad938-bada-43aa-d450-cbe568962d3d"},"source":["text[1111]"],"id":"related-museum","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'harry was not sure he could explain'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"diagnostic-windows","executionInfo":{"status":"ok","timestamp":1615639341295,"user_tz":-330,"elapsed":3127,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["from tqdm import tqdm\n","import random"],"id":"diagnostic-windows","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"combined-titanium","executionInfo":{"status":"ok","timestamp":1615179205848,"user_tz":-330,"elapsed":3360,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"5fa4e086-5b15-4f5e-b7d3-ed4945a4c4fc"},"source":["inp = []\n","out=[]\n","rand = [1,2,3]\n","for i in tqdm(text):\n","    temp = i.split()\n","    \n","    #removing a and also replacing with an (1 in every 5)\n","    if 'a' in temp:\n","        inp.append( (re.sub(r\"\\ba\\b\", \"\", i)).replace(\"  \", \" \") )\n","        out.append(i)\n","        if random.sample(rand,1)[0] == 3:\n","            inp.append( re.sub(r\"\\ba\\b\", \"an\", i) )\n","            out.append(i)\n","            \n","    #removing an and also replacing with a (1 in every 5)\n","    if 'an' in temp:\n","        inp.append( (re.sub(r\"\\ban\\b\", \"\", i)).replace(\"  \", \" \") )    #substituting and removing extra white space\n","        out.append(i)\n","        if random.sample(rand,1)[0] == 2:\n","            inp.append( re.sub(r\"\\ba\\b\", \"an\", i) )\n","            out.append(i)\n","            \n","    #removing the\n","    if 'the' in temp:\n","        inp.append( (re.sub(r\"\\bthe\\b\", \"\", i)).replace(\"  \", \" \") )\n","        out.append(i)\n","    \n","    # replacing 'their' with 'there' and viceversa\n","    if 'there' in temp:\n","        inp.append( re.sub(r\"\\bthere\\b\",\"their\", i) )\n","        out.append(i)\n","    \n","    if 'their' in temp:\n","        inp.append( re.sub(r\"\\btheir\\b\", \"there\", i) )\n","        out.append(i)\n","    \n","    #removing is\n","    if 'is' in temp:\n","        inp.append( (re.sub(r\"\\bis\\b\", \"\", i)).replace(\"  \", \" \") )\n","        out.append(i)\n","    #removing are\n","    if 'are' in temp:\n","        inp.append( (re.sub(r\"\\bare\\b\", \"\", i)).replace(\"  \", \" \") )\n","        out.append(i)\n","    #removing was\n","    if 'was' in temp:\n","        inp.append( (re.sub(r\"\\bwas\\b\", \"\", i)).replace(\"  \", \" \") )\n","        out.append(i)\n","    #removing were\n","    if 'were' in temp:\n","        inp.append( (re.sub(r\"\\bwere\\b\", \"\", i)).replace(\"  \", \" \") )\n","        out.append(i)\n","        \n","    #replacing than with then and vice versa\n","    if 'than' in temp:\n","        inp.append( re.sub(r\"\\bthan\\b\",\"then\", i) )\n","        out.append(i)\n","    \n","    if 'then' in temp:\n","        inp.append( re.sub(r\"\\bthen\\b\", \"than\", i) )\n","        out.append(i)\n","    \n","    \n","    \n","    #replacing 'may' with 'would' randomly one of out 5 times\n","    if 'may' in temp:\n","        if random.sample(rand,1)[0] == 1:\n","            inp.append( re.sub(r\"\\bmay\\b\", \"would\", i) )\n","            out.append(i)\n","    \n","    #replacing he/she with 'they' randomly one out of five times\n","    if 'he' in temp:\n","        if random.sample(rand,1)[0] == 1:\n","            inp.append( re.sub(r\"\\bhe\\b\", \"they\", i) )\n","            out.append(i)\n","    if 'she' in temp:\n","        if random.sample(rand,1)[0] == 2:\n","            inp.append( re.sub(r\"\\bshe\\b\", \"they\", i) )\n","            out.append(i)\n","\n","    "],"id":"combined-titanium","execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 254195/254195 [00:02<00:00, 120672.17it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"coordinated-eating","scrolled":true,"executionInfo":{"status":"ok","timestamp":1615179205851,"user_tz":-330,"elapsed":2955,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"745757cb-38d0-404b-d109-3c7d4de7d8af"},"source":["dataframe = pd.DataFrame({'encoder_input':inp, 'output':out})\n","print(dataframe.shape)\n","dataframe.head()"],"id":"coordinated-eating","execution_count":null,"outputs":[{"output_type":"stream","text":["(207330, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>mr dursley was the director of firm called gru...</td>\n","      <td>mr dursley was the director of a firm called g...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mr dursley was director of a firm called grunn...</td>\n","      <td>mr dursley was the director of a firm called g...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>mr dursley the director of a firm called grunn...</td>\n","      <td>mr dursley was the director of a firm called g...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>they did not think they could bear it if anyon...</td>\n","      <td>they did not think they could bear it if anyon...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>none of them noticed large tawny owl flutter p...</td>\n","      <td>none of them noticed a large tawny owl flutter...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       encoder_input                                             output\n","0  mr dursley was the director of firm called gru...  mr dursley was the director of a firm called g...\n","1  mr dursley was director of a firm called grunn...  mr dursley was the director of a firm called g...\n","2  mr dursley the director of a firm called grunn...  mr dursley was the director of a firm called g...\n","3  they did not think they could bear it if anyon...  they did not think they could bear it if anyon...\n","4  none of them noticed large tawny owl flutter p...  none of them noticed a large tawny owl flutter..."]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"meaning-trade"},"source":["### Examples random"],"id":"meaning-trade"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miniature-darkness","executionInfo":{"status":"ok","timestamp":1615179205852,"user_tz":-330,"elapsed":1078,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"ccbe93ce-f432-4fb6-f2ff-ab3e2c192b07"},"source":["inp[0], out[0]"],"id":"miniature-darkness","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('mr dursley was the director of firm called grunnings which made drills',\n"," 'mr dursley was the director of a firm called grunnings which made drills')"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aware-powell","executionInfo":{"status":"ok","timestamp":1615179206474,"user_tz":-330,"elapsed":732,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"2396ad1e-0890-4808-e5ea-7c529c87a084"},"source":["inp[6700], out[6700]"],"id":"aware-powell","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('it is very dangerous to assume you know what woman wants',\n"," 'it is very dangerous to assume you know what a woman wants')"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"divine-neighbor","executionInfo":{"status":"ok","timestamp":1615179207163,"user_tz":-330,"elapsed":1128,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"b3e93620-f5d5-4c6e-a0e6-d85cbeb15e80"},"source":["inp[3700], out[3700]"],"id":"divine-neighbor","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('they not birds', 'they are not birds')"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"recent-subject"},"source":["## Input-Output Pair with teacher forcing"],"id":"recent-subject"},{"cell_type":"code","metadata":{"id":"reported-response"},"source":["dataframe['decoder_input'] = '<start> ' + dataframe['output'].astype(str)\n","dataframe['decoder_output'] = dataframe['output'].astype(str) + ' <end>'\n","dataframe = dataframe.drop(['output'], axis=1)"],"id":"reported-response","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"reduced-dividend","executionInfo":{"status":"ok","timestamp":1615179209944,"user_tz":-330,"elapsed":1108,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"13e212d4-5cd1-45fc-a258-fcf5beeadc61"},"source":["dataframe.head()"],"id":"reduced-dividend","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>decoder_input</th>\n","      <th>decoder_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>mr dursley was the director of firm called gru...</td>\n","      <td>&lt;start&gt; mr dursley was the director of a firm ...</td>\n","      <td>mr dursley was the director of a firm called g...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mr dursley was director of a firm called grunn...</td>\n","      <td>&lt;start&gt; mr dursley was the director of a firm ...</td>\n","      <td>mr dursley was the director of a firm called g...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>mr dursley the director of a firm called grunn...</td>\n","      <td>&lt;start&gt; mr dursley was the director of a firm ...</td>\n","      <td>mr dursley was the director of a firm called g...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>they did not think they could bear it if anyon...</td>\n","      <td>&lt;start&gt; they did not think they could bear it ...</td>\n","      <td>they did not think they could bear it if anyon...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>none of them noticed large tawny owl flutter p...</td>\n","      <td>&lt;start&gt; none of them noticed a large tawny owl...</td>\n","      <td>none of them noticed a large tawny owl flutter...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       encoder_input  ...                                     decoder_output\n","0  mr dursley was the director of firm called gru...  ...  mr dursley was the director of a firm called g...\n","1  mr dursley was director of a firm called grunn...  ...  mr dursley was the director of a firm called g...\n","2  mr dursley the director of a firm called grunn...  ...  mr dursley was the director of a firm called g...\n","3  they did not think they could bear it if anyon...  ...  they did not think they could bear it if anyon...\n","4  none of them noticed large tawny owl flutter p...  ...  none of them noticed a large tawny owl flutter...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"miniature-endorsement"},"source":["### Example"],"id":"miniature-endorsement"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"experienced-minnesota","executionInfo":{"status":"ok","timestamp":1615179211497,"user_tz":-330,"elapsed":1203,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"87d14e81-9c39-4f6e-9961-00e101ea5e64"},"source":["dataframe['encoder_input'][0]"],"id":"experienced-minnesota","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'mr dursley was the director of firm called grunnings which made drills'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"legislative-reply","executionInfo":{"status":"ok","timestamp":1615179212970,"user_tz":-330,"elapsed":780,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"34790fc3-8420-49ae-a416-acf5dfd5d243"},"source":["dataframe['decoder_input'][0]"],"id":"legislative-reply","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> mr dursley was the director of a firm called grunnings which made drills'"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"sunset-tablet","executionInfo":{"status":"ok","timestamp":1615179213518,"user_tz":-330,"elapsed":879,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"0fbbaed5-f4cf-4884-aa37-8200ca37e634"},"source":["dataframe['decoder_output'][0]"],"id":"sunset-tablet","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'mr dursley was the director of a firm called grunnings which made drills <end>'"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"AiML2QMaoblP"},"source":["### train-test split"],"id":"AiML2QMaoblP"},{"cell_type":"code","metadata":{"id":"u-AMm7oEoeYl","executionInfo":{"status":"ok","timestamp":1615639349381,"user_tz":-330,"elapsed":3236,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["#train = pickle.load(open(\"C:/Users/rpris/Google Drive/Project_2/train.pkl\", 'rb'))\n","train = pickle.load(open('/content/drive/MyDrive/Project_2/train-reg.pkl', 'rb'))\n","\n","#validation = pickle.load(open(\"C:/Users/rpris/Google Drive/Project_2/validation.pkl\", 'rb'))\n","validation = pickle.load(open('/content/drive/MyDrive/Project_2/validation-reg.pkl', 'rb'))\n","\n","#test = pickle.load(open(\"C:/Users/rpris/Google Drive/Project_2/test.pkl\", 'rb'))\n","test = pickle.load(open('/content/drive/MyDrive/Project_2/test-reg.pkl', 'rb'))"],"id":"u-AMm7oEoeYl","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swcEh9Q5MaAD","executionInfo":{"status":"ok","timestamp":1615639352675,"user_tz":-330,"elapsed":2430,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"a9ad7105-c685-4337-b632-ebc4e209848a"},"source":["train.shape , validation.shape, test.shape"],"id":"swcEh9Q5MaAD","execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((123361, 3), (52869, 3), (31100, 3))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"political-lobby","executionInfo":{"status":"ok","timestamp":1615179220066,"user_tz":-330,"elapsed":1745,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"e35cba06-744b-4dfe-c3c3-2a4048f23b49"},"source":["from sklearn.model_selection import train_test_split\n","train, test = train_test_split(dataframe, test_size=0.15)\n","train, validation = train_test_split(train, test_size=0.3)\n","train.shape , validation.shape, test.shape"],"id":"political-lobby","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((123361, 3), (52869, 3), (31100, 3))"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"digital-travel"},"source":["# for one sentence we will be adding <end> token so that the tokanizer learns the word <end>\n","# with this we can use only one tokenizer for both decoder input and output\n","train.iloc[0]['decoder_input']= str(train.iloc[0]['decoder_input'])+' <end>'\n","train.iloc[0]['decoder_output']= str(train.iloc[0]['decoder_output'])+' <end>'"],"id":"digital-travel","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPeIv1KALSHM"},"source":["import pickle\n","pickle.dump(train,open(\"/content/drive/MyDrive/Project_2/train-reg.pkl\", \"wb\"))\n","pickle.dump(validation,open(\"/content/drive/MyDrive/Project_2/validation-reg.pkl\", \"wb\"))\n","pickle.dump(test,open(\"/content/drive/MyDrive/Project_2/test-reg.pkl\", \"wb\"))"],"id":"aPeIv1KALSHM","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imported-sapphire"},"source":["# Model"],"id":"imported-sapphire"},{"cell_type":"code","metadata":{"id":"incredible-today","executionInfo":{"status":"ok","timestamp":1615639362674,"user_tz":-330,"elapsed":4054,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"id":"incredible-today","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"physical-alabama"},"source":["## Tokenizing data"],"id":"physical-alabama"},{"cell_type":"code","metadata":{"id":"incomplete-extra","executionInfo":{"status":"ok","timestamp":1615639363740,"user_tz":-330,"elapsed":4101,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["token_perturbation = pickle.load(open(\"/content/drive/MyDrive/Project_2/token_pertubation-reg.pkl\",'rb'))\n","#token_perturbation = pickle.load(open(\"C:/Users/rpris/Google Drive/Project_2/token_pertubation.pkl\", 'rb'))\n","\n","token_correct = pickle.load(open(\"/content/drive/MyDrive/Project_2/token_correct-reg.pkl\", 'rb'))\n","#token_correct = pickle.load(open(\"C:/Users/rpris/Google Drive/Project_2/token_correct.pkl\", 'rb'))"],"id":"incomplete-extra","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"reverse-appeal"},"source":["token_perturbation = Tokenizer(filters='\"#$%&()*+-./:;=@[\\\\]^_`{|}~\\t\\n?,!')  # \n","token_perturbation.fit_on_texts(train['encoder_input'].values)\n","token_correct = Tokenizer(filters='\"#$%&()*+-./:;=@[\\\\]^_`{|}~\\t\\n?,!')   # \n","token_correct.fit_on_texts(train['decoder_input'].values)"],"id":"reverse-appeal","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"extensive-encyclopedia","executionInfo":{"status":"ok","timestamp":1615639365993,"user_tz":-330,"elapsed":3156,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"c373cb2e-94a5-4998-db88-15f242e31fe5"},"source":["v_perturbation = len(token_perturbation.word_index.keys()) +1   # +1 not\n","print(v_perturbation)\n","v_correct=len(token_correct.word_index.keys())  + 1\n","print(v_correct)"],"id":"extensive-encyclopedia","execution_count":11,"outputs":[{"output_type":"stream","text":["28465\n","28467\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"meaningful-relay","executionInfo":{"status":"ok","timestamp":1615536091158,"user_tz":-330,"elapsed":1045,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"0ba2f1a8-e44e-46ec-94bc-6d180fb9a9c1"},"source":["token_correct.word_index['<start>'] , token_correct.word_index['<end>']"],"id":"meaningful-relay","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 18896)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"descending-integrity"},"source":["pickle.dump(token_perturbation,open(\"/content/drive/MyDrive/Project_2/token_pertubation-reg.pkl\", \"wb\"))"],"id":"descending-integrity","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dress-cologne"},"source":["pickle.dump(token_correct, open(\"/content/drive/MyDrive/Project_2/token_correct-reg.pkl\", \"wb\"))"],"id":"dress-cologne","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"individual-finland"},"source":["## Creating embedding matrix from glove 300"],"id":"individual-finland"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ntMOa-vmeppo","executionInfo":{"status":"ok","timestamp":1615639373581,"user_tz":-330,"elapsed":1941,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"850f6ad8-8f4a-403d-bcec-958307eabf5f"},"source":["embedding_matrix = pickle.load(open('/content/drive/MyDrive/Project_2/embedding_matrix.pkl','rb'))\r\n","embedding_matrix.shape"],"id":"ntMOa-vmeppo","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28467, 300)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chicken-reminder","executionInfo":{"status":"ok","timestamp":1615637117375,"user_tz":-330,"elapsed":31199,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"eda69e6f-dd73-4eff-b672-cf88ac58c568"},"source":["embeddings_index = dict()\n","#f = open('C:\\\\Users\\\\rpris\\\\OneDrive\\\\Documents\\\\AI_data\\\\glove.6b\\\\glove.6B.300d.txt', encoding='utf8')\n","f = open(\"/content/drive/MyDrive/data_ml/glove.6B/glove.6B.300d.txt\", encoding='utf8')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","embedding_matrix = np.zeros((v_correct, 300))\n","for word, i in token_correct.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","embedding_matrix.shape"],"id":"chicken-reminder","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28467, 300)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"9WZMieP5eZY7"},"source":["pickle.dump(embedding_matrix, open('/content/drive/MyDrive/Project_2/embedding_matrix.pkl','wb'))"],"id":"9WZMieP5eZY7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHCFR96_fKbK"},"source":["***"],"id":"ZHCFR96_fKbK"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDZCXJRgfEWo","executionInfo":{"status":"ok","timestamp":1615639449612,"user_tz":-330,"elapsed":1992,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"190fe66a-4578-4596-e3a7-75cc4ac08295"},"source":["embedding_matrix_per = pickle.load(open('/content/drive/MyDrive/Project_2/embedding_matrix_per.pkl','rb'))\r\n","embedding_matrix_per.shape"],"id":"jDZCXJRgfEWo","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28465, 300)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQ43Hwcmjp5v","executionInfo":{"status":"ok","timestamp":1615639436701,"user_tz":-330,"elapsed":28256,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"0ac964d2-5427-4b2c-ca55-dd6951f0f06a"},"source":["embeddings_index = dict()\r\n","#f = open('C:\\\\Users\\\\rpris\\\\OneDrive\\\\Documents\\\\AI_data\\\\glove.6b\\\\glove.6B.300d.txt', encoding='utf8')\r\n","f = open(\"/content/drive/MyDrive/data_ml/glove.6B/glove.6B.300d.txt\", encoding='utf8')\r\n","for line in f:\r\n","    values = line.split()\r\n","    word = values[0]\r\n","    coefs = np.asarray(values[1:], dtype='float32')\r\n","    embeddings_index[word] = coefs\r\n","f.close()\r\n","\r\n","embedding_matrix_per = np.zeros((v_perturbation, 300))\r\n","for word, i in token_perturbation.word_index.items():\r\n","    embedding_vector = embeddings_index.get(word)\r\n","    if embedding_vector is not None:\r\n","        embedding_matrix_per[i] = embedding_vector\r\n","embedding_matrix_per.shape"],"id":"HQ43Hwcmjp5v","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28465, 300)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"Ll9dACVke_Lj","executionInfo":{"status":"ok","timestamp":1615639445045,"user_tz":-330,"elapsed":2650,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["pickle.dump(embedding_matrix_per, open('/content/drive/MyDrive/Project_2/embedding_matrix_per.pkl','wb'))"],"id":"Ll9dACVke_Lj","execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"committed-profit"},"source":["# Vanilla Encoder-Decoder"],"id":"committed-profit"},{"cell_type":"markdown","metadata":{"id":"beneficial-thursday"},"source":["## Creating architecture using model subclassing"],"id":"beneficial-thursday"},{"cell_type":"code","metadata":{"id":"_Gi3ZRZjjU1a"},"source":[],"id":"_Gi3ZRZjjU1a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"impressive-advancement"},"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.input_length = input_length\n","        self.enc_units= enc_units\n","        self.lstm_output = 0\n","        self.lstm_state_h=0\n","        self.lstm_state_c=0\n","        \n","    def build(self, input_shape):\n","        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n","                           mask_zero=True, name=\"embedding_layer_encoder\", input_shape=(self.vocab_size,))\n","        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n","        \n","    def call(self, input_sentances, training=True):\n","        input_embedd                        = self.embedding(input_sentances)\n","        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n","        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n","    def get_states(self):\n","        return self.lstm_state_h,self.lstm_state_c\n"],"id":"impressive-advancement","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dczIb6GwgHJc"},"source":["encoder_vanilla = pickle.dump(Encoder, open('/content/drive/MyDrive/Project_2/encodv.pkl','wb'))"],"id":"dczIb6GwgHJc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"effective-world"},"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.dec_units = dec_units\n","        self.input_length = input_length\n","    \n","    def build(self, input_shape):\n","        # we are using embedding_matrix weights and not training the embedding layer\n","        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n","                           mask_zero=True, name=\"embedding_layer_decoder\", weights=[embedding_matrix],input_shape=(self.vocab_size,))\n","        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n","        \n","    def call(self, target_sentances, state_h, state_c):\n","        target_embedd           = self.embedding(target_sentances)\n","        lstm_output, _,_        = self.lstm(target_embedd, initial_state=[state_h, state_c])\n","        return lstm_output"],"id":"effective-world","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cathedral-graham"},"source":["## Data pipeline"],"id":"cathedral-graham"},{"cell_type":"code","metadata":{"id":"macro-senator","executionInfo":{"status":"ok","timestamp":1615639461077,"user_tz":-330,"elapsed":4242,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["class Dataset:\n","    def __init__(self, data, token_perturbation, token_correct, max_len):\n","        self.encoder_inps = data['encoder_input'].values\n","        self.decoder_inps = data['decoder_input'].values\n","        self.decoder_outs = data['decoder_output'].values\n","        self.token_correct = token_correct\n","        self.token_perturbation = token_perturbation\n","        self.max_len = max_len\n","\n","    def __getitem__(self, i):\n","        self.encoder_seq = self.token_perturbation.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n","        self.decoder_inp_seq = self.token_correct.texts_to_sequences([self.decoder_inps[i]])\n","        self.decoder_out_seq = self.token_correct.texts_to_sequences([self.decoder_outs[i]])\n","\n","        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n","        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n","        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n","        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n","\n","    def __len__(self): # your model.fit_gen requires this function\n","        return len(self.encoder_inps)\n","\n"],"id":"macro-senator","execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"featured-offense","executionInfo":{"status":"ok","timestamp":1615639461078,"user_tz":-330,"elapsed":3895,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["class Dataloader(tf.keras.utils.Sequence):\n","    \n","    def __init__(self, dataset, batch_size=1):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.indexes = np.arange(len(self.dataset.encoder_inps))\n","\n","\n","    def __getitem__(self, i):\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n","\n","        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n","        \n","        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n","        \n","        return [batch[0],batch[1]],batch[2]\n","\n","    def __len__(self):  # your model.fit_gen requires this function\n","        return len(self.indexes) // self.batch_size\n","\n","    def on_epoch_end(self):\n","        self.indexes = np.random.permutation(self.indexes)"],"id":"featured-offense","execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"medium-letter"},"source":["## Model"],"id":"medium-letter"},{"cell_type":"code","metadata":{"id":"horizontal-links"},"source":["class vanilla_model(Model):\n","    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n","        super().__init__() \n","        self.encoder = Encoder(vocab_size=v_perturbation, embedding_dim=300, input_length=encoder_inputs_length, enc_units=256)\n","        self.decoder = Decoder(vocab_size=v_correct, embedding_dim=300, input_length=decoder_inputs_length, dec_units=256)\n","        self.dense   = Dense(output_vocab_size, activation='softmax')\n","        \n","        \n","    def call(self, data):\n","        input,output = data[0], data[1]\n","        encoder_output, encoder_h, encoder_c = self.encoder(input)\n","        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n","        output                               = self.dense(decoder_output)\n","        return output"],"id":"horizontal-links","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"neither-tennis","executionInfo":{"status":"ok","timestamp":1614501970370,"user_tz":-330,"elapsed":1433,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"779af52f-0417-49f7-ebc8-bd125cb443fe"},"source":["train_dataset = Dataset(train, token_perturbation, token_correct, 16)\n","validation_dataset  = Dataset(validation, token_perturbation, token_correct, 16)\n","test_dataset = Dataset(test, token_perturbation, token_correct, 16)\n","\n","train_dataloader = Dataloader(train_dataset, batch_size=512)\n","validation_dataloader = Dataloader(validation_dataset, batch_size=512)\n","test_dataloader = Dataloader(test_dataset, batch_size=512)\n","\n","\n","print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"],"id":"neither-tennis","execution_count":null,"outputs":[{"output_type":"stream","text":["(512, 16) (512, 16) (512, 16)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"postal-portrait"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"id":"postal-portrait","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"earlier-africa"},"source":["vanilla = vanilla_model(encoder_inputs_length=16,decoder_inputs_length=16,output_vocab_size=v_correct)\n","optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n","vanilla.compile(optimizer= optimizer, loss= loss_function, metrics=['accuracy'])"],"id":"earlier-africa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fewer-career"},"source":["import datetime\n","\n","logfile = \"/content/drive/MyDrive/Project_2/logs/vanilla/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#logfile = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\logs\\\\vanilla\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tfboard = tf.keras.callbacks.TensorBoard(log_dir=logfile, histogram_freq=1, write_graph=True)\n","\n","chkfile = \"/content/drive/MyDrive/Project_2/wts/vanilla/weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n","#chkfile2 = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\wts\\\\vanilla\\\\weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n","chkpt = tf.keras.callbacks.ModelCheckpoint(chkfile, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=0, mode='min')\n","\n","stp = tf.keras.callbacks.EarlyStopping(patience=7, monitor='val_loss')"],"id":"fewer-career","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUqzPMbcjsc4"},"source":["vanilla.load_weights(\"/content/drive/MyDrive/Project_2/wts/vanilla/weights-65-0.5898.hdf5\")"],"id":"sUqzPMbcjsc4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LNgdV8GlsDy","executionInfo":{"status":"ok","timestamp":1614502040409,"user_tz":-330,"elapsed":754,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"e704f115-9f5b-4435-d44d-e7133e3b81c5"},"source":["vanilla.summary()"],"id":"8LNgdV8GlsDy","execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vanilla_model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder (Encoder)            multiple                  8705768   \n","_________________________________________________________________\n","decoder (Decoder)            multiple                  8705168   \n","_________________________________________________________________\n","dense (Dense)                multiple                  6968812   \n","=================================================================\n","Total params: 24,379,748\n","Trainable params: 16,244,948\n","Non-trainable params: 8,134,800\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ambient-improvement","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614496852856,"user_tz":-330,"elapsed":679113,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"2dfa3d02-cffc-45ac-b475-889fa223f32e"},"source":["train_steps=train.shape[0]//512\n","valid_steps=validation.shape[0]//512\n","vanilla.fit(train_dataloader, steps_per_epoch=train_steps, epochs=100, validation_data=validation_dataloader,\n","                      validation_steps=valid_steps, callbacks=[stp, chkpt, tfboard], initial_epoch=57)"],"id":"ambient-improvement","execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 58/100\n","115/115 [==============================] - 46s 397ms/step - loss: 0.1084 - accuracy: 0.9452 - val_loss: 0.6000 - val_accuracy: 0.7373\n","Epoch 59/100\n","115/115 [==============================] - 44s 382ms/step - loss: 0.1012 - accuracy: 0.9502 - val_loss: 0.5956 - val_accuracy: 0.7411\n","Epoch 60/100\n","115/115 [==============================] - 43s 377ms/step - loss: 0.0970 - accuracy: 0.9526 - val_loss: 0.5958 - val_accuracy: 0.7404\n","Epoch 61/100\n","115/115 [==============================] - 43s 378ms/step - loss: 0.0936 - accuracy: 0.9543 - val_loss: 0.5913 - val_accuracy: 0.7428\n","Epoch 62/100\n","115/115 [==============================] - 44s 380ms/step - loss: 0.0892 - accuracy: 0.9568 - val_loss: 0.5916 - val_accuracy: 0.7456\n","Epoch 63/100\n","115/115 [==============================] - 43s 378ms/step - loss: 0.0844 - accuracy: 0.9597 - val_loss: 0.5918 - val_accuracy: 0.7461\n","Epoch 64/100\n","115/115 [==============================] - 43s 377ms/step - loss: 0.0814 - accuracy: 0.9610 - val_loss: 0.5921 - val_accuracy: 0.7487\n","Epoch 65/100\n","115/115 [==============================] - 43s 376ms/step - loss: 0.0769 - accuracy: 0.9637 - val_loss: 0.5898 - val_accuracy: 0.7489\n","Epoch 66/100\n","115/115 [==============================] - 44s 379ms/step - loss: 0.0726 - accuracy: 0.9664 - val_loss: 0.5943 - val_accuracy: 0.7492\n","Epoch 67/100\n","115/115 [==============================] - 43s 376ms/step - loss: 0.0690 - accuracy: 0.9683 - val_loss: 0.5959 - val_accuracy: 0.7502\n","Epoch 68/100\n","115/115 [==============================] - 43s 378ms/step - loss: 0.0661 - accuracy: 0.9699 - val_loss: 0.6003 - val_accuracy: 0.7502\n","Epoch 69/100\n","115/115 [==============================] - 43s 376ms/step - loss: 0.0652 - accuracy: 0.9696 - val_loss: 0.5961 - val_accuracy: 0.7520\n","Epoch 70/100\n","115/115 [==============================] - 43s 376ms/step - loss: 0.0628 - accuracy: 0.9709 - val_loss: 0.5983 - val_accuracy: 0.7531\n","Epoch 71/100\n","115/115 [==============================] - 43s 376ms/step - loss: 0.0605 - accuracy: 0.9721 - val_loss: 0.5948 - val_accuracy: 0.7566\n","Epoch 72/100\n","115/115 [==============================] - 43s 376ms/step - loss: 0.0548 - accuracy: 0.9762 - val_loss: 0.5964 - val_accuracy: 0.7592\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7ff3dc69ee10>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"I6nqPyylym-v"},"source":["### Logs"],"id":"I6nqPyylym-v"},{"cell_type":"markdown","metadata":{"id":"x3tNnUkqaiRY"},"source":["#### After epoch 65, the model reaches its minimum loss"],"id":"x3tNnUkqaiRY"},{"cell_type":"markdown","metadata":{"id":"77rCLIC_y5C_"},"source":["#### Orange- Train\r\n","#### Blue- Validation"],"id":"77rCLIC_y5C_"},{"cell_type":"markdown","metadata":{"id":"OFCPYC55yrFA"},"source":["<img src =\"https://imgur.com/assAhwy.png\">\r\n","<img src =\"https://imgur.com/hkMReZX.png\" >"],"id":"OFCPYC55yrFA"},{"cell_type":"markdown","metadata":{"id":"5SFjlz-AG1t4"},"source":["## Inference"],"id":"5SFjlz-AG1t4"},{"cell_type":"code","metadata":{"id":"DZ_QeXvxG1Wu"},"source":["def inference(enc_inp,dec_inp):\n","    \n","    translation=\"\"\n","\n","    e_input=[]\n","    for i in enc_inp.split():\n","        if token_perturbation.word_index.get(i) == None:\n","            e_input.append(0)\n","        else:\n","            e_input.append(token_perturbation.word_index.get(i))\n","    \n","    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n","    \n","    \n","    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n","    \n","    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n","    \n","    #decoder input\n","    d_input=[]\n","    for i in dec_inp.split():\n","        if token_correct.word_index.get(i) == None:\n","            d_input.append(0)\n","        else:\n","            d_input.append(token_correct.word_index.get(i))\n","\n","    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n","    \n","    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n","\n","    for word in prediction[0]:\n","        word = token_correct.index_word[tf.argmax(word).numpy()]\n","        if word == \"<end>\":\n","            return translation\n","        translation += word + \" \"\n","    \n","    return translation\n","    "],"id":"DZ_QeXvxG1Wu","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"Bmz-pIVVHKCY","executionInfo":{"status":"ok","timestamp":1614502155728,"user_tz":-330,"elapsed":23231,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"4b8780a7-e496-4ade-e8e4-e4a5ae806f6b"},"source":["sample_train = train.sample(1000)\n","result = []\n","i=0\n","for enc_inp,dec_inp,_ in sample_train.values:\n","    pred = inference(enc_inp,dec_inp)\n","    result.append(pred)\n","\n","sample_train['correct_output'] = sample_train['decoder_output']\n","sample_train['predicted_output'] = result\n","\n","sample_train = sample_train.drop(['decoder_input', 'decoder_output'], axis=1)\n","\n","sample_train.head(10)"],"id":"Bmz-pIVVHKCY","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>correct_output</th>\n","      <th>predicted_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>45193</th>\n","      <td>it was 25 signs he great in bed</td>\n","      <td>it was 25 signs he is great in bed &lt;end&gt;</td>\n","      <td>it was 25 signs he is great in bed</td>\n","    </tr>\n","    <tr>\n","      <th>56974</th>\n","      <td>we good?</td>\n","      <td>are we good? &lt;end&gt;</td>\n","      <td>are we good?</td>\n","    </tr>\n","    <tr>\n","      <th>78004</th>\n","      <td>look, you said you not feeling great</td>\n","      <td>look, you said you were not feeling great &lt;end&gt;</td>\n","      <td>look, you said you were not feeling great</td>\n","    </tr>\n","    <tr>\n","      <th>20015</th>\n","      <td>what his name</td>\n","      <td>what was his name  &lt;end&gt;</td>\n","      <td>what is his name</td>\n","    </tr>\n","    <tr>\n","      <th>15078</th>\n","      <td>let is head out to tarmac</td>\n","      <td>let is head out to the tarmac &lt;end&gt;</td>\n","      <td>let is head out to the tarmac</td>\n","    </tr>\n","    <tr>\n","      <th>1170</th>\n","      <td>and this ron, our brother</td>\n","      <td>and this is ron, our brother &lt;end&gt;</td>\n","      <td>and this is ron, our brother</td>\n","    </tr>\n","    <tr>\n","      <th>32331</th>\n","      <td>he after me</td>\n","      <td>he is after me &lt;end&gt;</td>\n","      <td>he is after me</td>\n","    </tr>\n","    <tr>\n","      <th>76570</th>\n","      <td>the important thing not to panic</td>\n","      <td>the important thing is not to panic &lt;end&gt;</td>\n","      <td>the important thing is not to panic</td>\n","    </tr>\n","    <tr>\n","      <th>80471</th>\n","      <td>i do not know how many other relatives he got ...</td>\n","      <td>i do not know how many other relatives he is g...</td>\n","      <td>i do not know how many other relatives he is g...</td>\n","    </tr>\n","    <tr>\n","      <th>40691</th>\n","      <td>quot well i believe in god, and only thing tha...</td>\n","      <td>quot well i believe in god, and the only thing...</td>\n","      <td>well i i believe in god, the the only thing th...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           encoder_input  ...                                   predicted_output\n","45193                    it was 25 signs he great in bed  ...                it was 25 signs he is great in bed \n","56974                                           we good?  ...                                      are we good? \n","78004               look, you said you not feeling great  ...         look, you said you were not feeling great \n","20015                                     what his name   ...                                  what is his name \n","15078                          let is head out to tarmac  ...                     let is head out to the tarmac \n","1170                           and this ron, our brother  ...                      and this is ron, our brother \n","32331                                        he after me  ...                                    he is after me \n","76570                   the important thing not to panic  ...               the important thing is not to panic \n","80471  i do not know how many other relatives he got ...  ...  i do not know how many other relatives he is g...\n","40691  quot well i believe in god, and only thing tha...  ...  well i i believe in god, the the only thing th...\n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"GCjPs2xhwoui","executionInfo":{"status":"ok","timestamp":1614502179424,"user_tz":-330,"elapsed":45967,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"21fb572d-ef0c-4041-af64-3e709fbf8170"},"source":["sample_validation = validation.sample(1000)\n","result = []\n","for enc_inp,dec_inp,_ in sample_validation.values:\n","    pred = inference(enc_inp,dec_inp)\n","    result.append(pred)\n","\n","sample_validation['correct_output'] = sample_validation['decoder_output']\n","sample_validation['predicted_output'] = result\n","sample_validation = sample_validation.drop(['decoder_input', 'decoder_output'], axis=1)\n","sample_validation.head(10)"],"id":"GCjPs2xhwoui","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>correct_output</th>\n","      <th>predicted_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>45485</th>\n","      <td>well, it not exactly the term i would have cho...</td>\n","      <td>well, it is not exactly the term i would have ...</td>\n","      <td>it it is not sick, i me, i can have as as as c...</td>\n","    </tr>\n","    <tr>\n","      <th>43127</th>\n","      <td>you need anything, help of any kind, gimme holler</td>\n","      <td>you need anything, help of any kind, gimme a h...</td>\n","      <td>you need anything, help any any kind, gimme a ...</td>\n","    </tr>\n","    <tr>\n","      <th>58875</th>\n","      <td>tasting and enjoying life the only thing of va...</td>\n","      <td>tasting and enjoying life is the only thing of...</td>\n","      <td>and and the the were the only our must leavin ...</td>\n","    </tr>\n","    <tr>\n","      <th>71081</th>\n","      <td>why you carrying the dog?</td>\n","      <td>why are you carrying the dog? &lt;end&gt;</td>\n","      <td>why are you carrying the dog?</td>\n","    </tr>\n","    <tr>\n","      <th>48388</th>\n","      <td>hey, angela, sal and i go back long way</td>\n","      <td>hey, angela, sal and i go back a long way &lt;end&gt;</td>\n","      <td>yeah, thank i i just go back down back way</td>\n","    </tr>\n","    <tr>\n","      <th>72697</th>\n","      <td>that is what you giving me?</td>\n","      <td>that is what you are giving me? &lt;end&gt;</td>\n","      <td>that is what you were giving me?</td>\n","    </tr>\n","    <tr>\n","      <th>47689</th>\n","      <td>what is code, travers?</td>\n","      <td>what is the code, travers? &lt;end&gt;</td>\n","      <td>what is the code, travers?</td>\n","    </tr>\n","    <tr>\n","      <th>76278</th>\n","      <td>someone names an lot of warts on line two</td>\n","      <td>someone names a lot of warts on line two &lt;end&gt;</td>\n","      <td>someone a a lot of the the the the</td>\n","    </tr>\n","    <tr>\n","      <th>26049</th>\n","      <td>if another ship had been beneath us klingons w...</td>\n","      <td>if another ship had been beneath us the klingo...</td>\n","      <td>if a a is a locked out, would would could neve...</td>\n","    </tr>\n","    <tr>\n","      <th>73197</th>\n","      <td>what the problem, molly?</td>\n","      <td>what is the problem, molly? &lt;end&gt;</td>\n","      <td>what is the problem, molly?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           encoder_input  ...                                   predicted_output\n","45485  well, it not exactly the term i would have cho...  ...  it it is not sick, i me, i can have as as as c...\n","43127  you need anything, help of any kind, gimme holler  ...  you need anything, help any any kind, gimme a ...\n","58875  tasting and enjoying life the only thing of va...  ...  and and the the were the only our must leavin ...\n","71081                          why you carrying the dog?  ...                     why are you carrying the dog? \n","48388            hey, angela, sal and i go back long way  ...        yeah, thank i i just go back down back way \n","72697                        that is what you giving me?  ...                  that is what you were giving me? \n","47689                             what is code, travers?  ...                        what is the code, travers? \n","76278          someone names an lot of warts on line two  ...                someone a a lot of the the the the \n","26049  if another ship had been beneath us klingons w...  ...  if a a is a locked out, would would could neve...\n","73197                           what the problem, molly?  ...                       what is the problem, molly? \n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"WGZ3ksUkxm4X","executionInfo":{"status":"ok","timestamp":1614502203137,"user_tz":-330,"elapsed":69064,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"0b911e45-15c5-4243-aa8a-4d4fdddbebb3"},"source":["sample_test = test.sample(1000)\n","result = []\n","for enc_inp,dec_inp,_ in sample_test.values:\n","    pred = inference(enc_inp,dec_inp)\n","    result.append(pred)\n","sample_test['correct_output'] = sample_test['decoder_output']\n","sample_test['predicted_output'] = result\n","sample_test = sample_test.drop(['decoder_input', 'decoder_output'], axis=1)\n","sample_test.head(10)"],"id":"WGZ3ksUkxm4X","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>correct_output</th>\n","      <th>predicted_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>77691</th>\n","      <td>we in the spite</td>\n","      <td>we are in the spite &lt;end&gt;</td>\n","      <td>we are in a waste</td>\n","    </tr>\n","    <tr>\n","      <th>33526</th>\n","      <td>he said, yes, doctor</td>\n","      <td>he said, yes, the doctor &lt;end&gt;</td>\n","      <td>he said, yes, the doctor</td>\n","    </tr>\n","    <tr>\n","      <th>59705</th>\n","      <td>you will probably be richest man in america so...</td>\n","      <td>you will probably be the richest man in americ...</td>\n","      <td>you will probably be a most in in helping to t...</td>\n","    </tr>\n","    <tr>\n","      <th>77919</th>\n","      <td>i will be right their</td>\n","      <td>i will be right there &lt;end&gt;</td>\n","      <td>i will be right there</td>\n","    </tr>\n","    <tr>\n","      <th>42966</th>\n","      <td>she a little nuts, but she a good girl i think</td>\n","      <td>she is a little nuts, but she is a good girl i...</td>\n","      <td>she is a little nuts, but she is a good girl i...</td>\n","    </tr>\n","    <tr>\n","      <th>67483</th>\n","      <td>this girl a godsend</td>\n","      <td>this girl is a godsend &lt;end&gt;</td>\n","      <td>this girl is a godsend</td>\n","    </tr>\n","    <tr>\n","      <th>12904</th>\n","      <td>you are right</td>\n","      <td>you were right &lt;end&gt;</td>\n","      <td>you were right</td>\n","    </tr>\n","    <tr>\n","      <th>2450</th>\n","      <td>there a note!</td>\n","      <td>there is a note! &lt;end&gt;</td>\n","      <td>there is a note!</td>\n","    </tr>\n","    <tr>\n","      <th>37161</th>\n","      <td>it perhaps strange that we both should be in l...</td>\n","      <td>it is perhaps strange that we both should be i...</td>\n","      <td>it is perhaps strange than we can you, be inte...</td>\n","    </tr>\n","    <tr>\n","      <th>11335</th>\n","      <td>that you, jeffrey?</td>\n","      <td>is that you, jeffrey? &lt;end&gt;</td>\n","      <td>is that you, jeffrey?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           encoder_input  ...                                   predicted_output\n","77691                                    we in the spite  ...                                 we are in a waste \n","33526                               he said, yes, doctor  ...                          he said, yes, the doctor \n","59705  you will probably be richest man in america so...  ...  you will probably be a most in in helping to t...\n","77919                              i will be right their  ...                             i will be right there \n","42966     she a little nuts, but she a good girl i think  ...  she is a little nuts, but she is a good girl i...\n","67483                                this girl a godsend  ...                            this girl is a godsend \n","12904                                      you are right  ...                                    you were right \n","2450                                       there a note!  ...                                  there is a note! \n","37161  it perhaps strange that we both should be in l...  ...  it is perhaps strange than we can you, be inte...\n","11335                                 that you, jeffrey?  ...                             is that you, jeffrey? \n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"5tOLeTbagE70"},"source":["## BLEU Score"],"id":"5tOLeTbagE70"},{"cell_type":"code","metadata":{"id":"m1HI-iEOgE71"},"source":["import nltk.translate.bleu_score as bleu"],"id":"m1HI-iEOgE71","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xkNoVAuigE72","executionInfo":{"status":"ok","timestamp":1614502764541,"user_tz":-330,"elapsed":1372,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"dc506e66-b23f-4e88-dd9f-4e14465951ef"},"source":["train_bleu = []\n","for enc_inp,correct,pred in sample_train.values:\n","    \n","    correct = correct.split()[:-1]    #removing end\n","    pred = pred.split()\n","    if len(correct) == len(pred):\n","        train_bleu.append(bleu.sentence_bleu([correct],pred))\n","print(\"BLEU Score of train dataset is\",sum(train_bleu)/len(train_bleu))"],"id":"xkNoVAuigE72","execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU Score of train dataset is 0.9444792707470997\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"OC6AzgXm3TpF"},"source":["<font size=4 color='red'> BLEU Score of train dataset is 0.9444792707470997"],"id":"OC6AzgXm3TpF"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W5T5yJqv7op5","executionInfo":{"status":"ok","timestamp":1614502645500,"user_tz":-330,"elapsed":1149,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"304a3a2d-4d4b-42b8-f37a-95dfb28c8e31"},"source":["validation_bleu = []\n","for enc_inp,correct,pred in sample_validation.values:\n","    \n","    correct = correct.split()[:-1]    #removing end\n","    pred = pred.split()\n","    if len(correct) == len(pred):\n","        validation_bleu.append(bleu.sentence_bleu([correct],pred))\n","print(\"BLEU Score of validation dataset is\",sum(validation_bleu)/len(validation_bleu))"],"id":"W5T5yJqv7op5","execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU Score of validation dataset is 0.7513216789262608\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"uOSCf0HC3b8d"},"source":["<font size=4 color='red'> BLEU Score of validation dataset is 0.7513216789262608"],"id":"uOSCf0HC3b8d"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYfUGIRr2422","executionInfo":{"status":"ok","timestamp":1614502689973,"user_tz":-330,"elapsed":1198,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"5ed75af2-73e4-4f81-8dcb-7e9baa6d5a64"},"source":["test_bleu = []\r\n","for enc_inp,correct,pred in sample_test.values:\r\n","    \r\n","    correct = correct.split()[:-1]    #removing end\r\n","    pred = pred.split()\r\n","    if len(correct) == len(pred):\r\n","        test_bleu.append(bleu.sentence_bleu([correct],pred))\r\n","print(\"BLEU Score of test dataset is\",sum(test_bleu)/len(test_bleu))"],"id":"DYfUGIRr2422","execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU Score of test dataset is 0.7486602027685026\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qWN_LtMS3gtI"},"source":["<font size=4 color='red'> BLEU Score of test dataset is 0.7486602027685026"],"id":"qWN_LtMS3gtI"},{"cell_type":"markdown","metadata":{"id":"declared-poster"},"source":["# Attention Mechanism"],"id":"declared-poster"},{"cell_type":"code","metadata":{"id":"rDx4sFt--cu0","executionInfo":{"status":"ok","timestamp":1615639470879,"user_tz":-330,"elapsed":2205,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["import tensorflow as tf\r\n","from tensorflow.keras import layers\r\n","from tensorflow.keras.layers import Layer\r\n","from tensorflow.keras import initializers, regularizers, constraints"],"id":"rDx4sFt--cu0","execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suitable-animation"},"source":["## Architecture"],"id":"suitable-animation"},{"cell_type":"markdown","metadata":{"id":"approved-emergency"},"source":["### Encoder"],"id":"approved-emergency"},{"cell_type":"code","metadata":{"id":"living-testament"},"source":["class Encoder(tf.keras.Model):\n","    '''\n","    Encoder model -- That takes a input sequence and returns output sequence\n","    '''\n","\n","    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n","        super().__init__()\n","\n","        self.inp_vocab_size = inp_vocab_size\n","        self.embedding_size = embedding_size\n","        self.lstm_size = lstm_size\n","        self.input_length = input_length\n","\n","    \n","    #def build(self, input_shape):\n","        self.embedd = Embedding(input_dim = self.inp_vocab_size, output_dim = self.embedding_size, input_length=self.input_length,\n","                                weights = [embedding_matrix_per], mask_zero=True)\n","        \n","        self.encod_lstm = LSTM(units = self.lstm_size, return_sequences=True, return_state=True, \n","                               name=\"Encoder_Attention\", activation = 'tanh',recurrent_activation ='sigmoid', \n","                               recurrent_dropout = 0, unroll=False, use_bias=True, \n","                               kernel_regularizer= regularizers.l2(1e-6))\n","\n","    def call(self,input_sequence, training=True):\n","        embeddings = self.embedd(input_sequence)\n","\n","        encod_out, encod_h, encod_c = self.encod_lstm(embeddings)\n","\n","        return encod_out, encod_h, encod_c\n","\n","    def initialize_states(self,batch_size):\n","\n","        hidden = np.zeros((batch_size, self.lstm_units))\n","\n","        cell = np.zeros((batch_size, self.lstm_units))\n","        \n","        return hidden, cell\n","\n","    def get_config(self):\n","        '''Config'''\n","\n","        config = { 'inp_vocab_size': self.inp_vocab_size,\n","                  'embedding_size': self.embedding_size,\n","                  'lstm_size': self.lstm_size,\n","                  'input_length': self.input_length\n","        }\n","\n","        base_config = super(Encoder, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)"],"id":"living-testament","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"immediate-opinion"},"source":["### Attention"],"id":"immediate-opinion"},{"cell_type":"markdown","metadata":{"id":"eOt8jUJ00Rcf"},"source":["https://github.com/UdiBhaskar/TfKeras-Custom-Layers/blob/master/Seq2Seq/clayers.py"],"id":"eOt8jUJ00Rcf"},{"cell_type":"markdown","metadata":{"id":"IepvlFDR13vt"},"source":["#### Monotonic Bahanau Attention"],"id":"IepvlFDR13vt"},{"cell_type":"code","metadata":{"id":"xhARl4TiuFnF","executionInfo":{"status":"ok","timestamp":1615639478612,"user_tz":-330,"elapsed":1368,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["def _attention_score(dec_ht,\r\n","                     enc_hs,\r\n","                     attention_type,\r\n","                     weightwa=None,\r\n","                     weightua=None,\r\n","                     weightva=None):\r\n","    if attention_type == 'bahdanau':\r\n","        score = weightva(tf.nn.tanh(weightwa(dec_ht) + weightua(enc_hs)))\r\n","        score = tf.squeeze(score, [2])\r\n","    elif attention_type == 'dot':\r\n","        score = tf.matmul(dec_ht, enc_hs, transpose_b=True)\r\n","        score = tf.squeeze(score, 1)\r\n","    elif attention_type == 'general':\r\n","        score = weightwa(enc_hs)\r\n","        score = tf.matmul(dec_ht, score, transpose_b=True)\r\n","        score = tf.squeeze(score, 1)\r\n","    elif attention_type == 'concat':\r\n","        dec_ht = tf.tile(dec_ht, [1, enc_hs.shape[1], 1])\r\n","        score = weightva(tf.nn.tanh(weightwa(tf.concat((dec_ht, enc_hs), axis=-1))))\r\n","        score = tf.squeeze(score, 2)\r\n","    return score\r\n"],"id":"xhARl4TiuFnF","execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ow-nG3tewlAu","executionInfo":{"status":"ok","timestamp":1615639478612,"user_tz":-330,"elapsed":855,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["def _monotonic_attetion(probabilities, attention_prev, mode):\r\n","\r\n","    if mode == 'hard':\r\n","        #Remove any probabilities before the index chosen last time step\r\n","        probabilities = probabilities*tf.cumsum(attention_prev, axis=1)\r\n","        attention = probabilities*tf.math.cumprod(1-probabilities, axis=1, exclusive=True)\r\n","    elif mode == 'recursive':\r\n","        batch_size = tf.shape(input=probabilities)[0]\r\n","        shifted_1mp_probabilities = tf.concat([tf.ones((batch_size, 1)),\\\r\n","            1 - probabilities[:, :-1]], 1)\r\n","        attention = probabilities*tf.transpose(a=tf.scan(lambda x, yz: tf.reshape(yz[0]*x + yz[1],\\\r\n","            (batch_size,)), [tf.transpose(a=shifted_1mp_probabilities),\\\r\n","                tf.transpose(a=attention_prev)], tf.zeros((batch_size,))))\r\n","    elif mode == 'parallel':\r\n","        cumprod_1mp_probabilities = tf.exp(tf.cumsum(tf.math.log(tf.clip_by_value(1-probabilities,\\\r\n","            1e-10, 1)), axis=1, exclusive=True))\r\n","        attention = probabilities*cumprod_1mp_probabilities*tf.cumsum(attention_prev/\\\r\n","            tf.clip_by_value(cumprod_1mp_probabilities, 1e-10, 1.), axis=1)\r\n","    else:\r\n","        raise ValueError(\"Mode must be 'hard', 'parallel' or 'recursive' \")\r\n","\r\n","    return attention\r\n"],"id":"ow-nG3tewlAu","execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"89ST83W4rXoo"},"source":["class MonotonicBahaAtt(tf.keras.layers.Layer):\r\n","    def __init__(self, units,\r\n","                 mode,\r\n","                 return_aweights=False,\r\n","                 scaling_factor=None,\r\n","                 noise_std=0,\r\n","                 weights_initializer='he_normal',\r\n","                 bias_initializer='zeros',\r\n","                 **kwargs):\r\n","        \r\n","        if 'name' not in kwargs:\r\n","            kwargs['name'] = \"\"\r\n","            \r\n","        super(MonotonicBahaAtt, self).__init__(**kwargs)\r\n","        self.units = units\r\n","        self.mode = mode\r\n","        self.scaling_factor = scaling_factor\r\n","        self.noise_std = noise_std\r\n","        self.weights_initializer = initializers.get(weights_initializer)\r\n","        self.bias_initializer = initializers.get(bias_initializer)\r\n","        self.weights_regularizer = regularizers.l2(1e-2)\r\n","    \r\n","    def build(self, input_shape):\r\n","        self._wa = layers.Dense(self.units, use_bias=False,\\\r\n","            kernel_initializer=self.weights_initializer, bias_initializer=self.bias_initializer,\\\r\n","                kernel_regularizer= self.weights_regularizer, name=self.name+\"Wa\")\r\n","        \r\n","        self._ua = layers.Dense(self.units,\\\r\n","            kernel_initializer=self.weights_initializer, bias_initializer=self.bias_initializer,\\\r\n","                kernel_regularizer= self.weights_regularizer, name=self.name+\"Ua\")\r\n","        \r\n","        self._va = layers.Dense(1, use_bias=False, kernel_initializer=self.weights_initializer,\\\r\n","            kernel_regularizer= self.weights_regularizer,bias_initializer=self.bias_initializer, name=self.name+\"Va\")\r\n","        \r\n","        \r\n","    def call(self, decoder_hidden_state, encoder_outputs, prev_attention, training=True):\r\n","\r\n","        encoder_outputs, decoder_hidden_state = tf.cast(encoder_outputs, tf.float32), \\\r\n","            tf.cast(decoder_hidden_state, tf.float32)\r\n","        \r\n","        dec_hidden_with_time_axis = tf.expand_dims(decoder_hidden_state, 1)\r\n","\r\n","        # score shape == (batch_size, max_length)\r\n","        score = _attention_score(dec_ht=dec_hidden_with_time_axis, enc_hs=encoder_outputs,\\\r\n","                    attention_type='bahdanau', weightwa=self._wa,\\\r\n","                        weightua=self._ua, weightva=self._va)\r\n","        \r\n","        if self.scaling_factor is not None:\r\n","            score = score/tf.sqrt(self.scaling_factor)\r\n","\r\n","        if training:\r\n","            if self.noise_std > 0:\r\n","                random_noise = tf.random.normal(shape=tf.shape(input=score), mean=0,\\\r\n","                    stddev=self.noise_std, dtype=score.dtype, seed=self.seed)\r\n","                score = score + random_noise\r\n","\r\n","        if self.mode == 'hard':\r\n","            probabilities = tf.cast(score > 0, score.dtype)\r\n","        else:\r\n","            probabilities = tf.sigmoid(score)\r\n","\r\n","        \r\n","        attention_weights = _monotonic_attetion(probabilities, prev_attention, self.mode)\r\n","        attention_weights = tf.expand_dims(attention_weights, 1)\r\n","\r\n","        #context_vector shape (batch_size, hidden_size)\r\n","        context_vector = tf.matmul(attention_weights, encoder_outputs)\r\n","        context_vector = tf.squeeze(context_vector, 1, name=\"context_vector\")\r\n","\r\n","        return context_vector, tf.squeeze(attention_weights, 1, name='attention_weights')\r\n","\r\n","    def get_config(self):\r\n","        config = {'units': self.units,\r\n","                  'mode': self.mode,\r\n","                  'scaling_factor': self.scaling_factor,\r\n","                  'noise_std': self.noise_std,\r\n","                  'weights_initializer': initializers.serialize(self.weights_initializer),\r\n","                  'bias_initializer': initializers.serialize(self.bias_initializer),\r\n","                  'weights_regularizer': self.weights_regularizer \r\n","                  }\r\n","\r\n","        base_config = super(MonotonicBahaAtt, self).get_config()\r\n","        return dict(list(base_config.items()) + list(config.items()))"],"id":"89ST83W4rXoo","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"welcome-seafood"},"source":["### One step decoder"],"id":"welcome-seafood"},{"cell_type":"code","metadata":{"id":"structural-strengthening"},"source":["class OneStepDecoder(tf.keras.Model):\n","\n","    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n","        super().__init__()\n","        \n","        self.tar_vocab_size = tar_vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.input_length = input_length\n","        self.dec_units = dec_units\n","        self.score_fun = score_fun\n","        self.att_units = att_units\n","\n","    \n","    #def build(self, input_shape):\n","\n","        self.decod_embedd = Embedding(input_dim = self.tar_vocab_size, output_dim = self.embedding_dim,input_length = self.input_length,\n","                                      weights = [embedding_matrix] , name=\"embedding_layer_onestepdecoder\", mask_zero=True)\n","        \n","        self.decod_LSTM = LSTM(units = self.dec_units, return_state=True, activation = 'tanh', \n","                               recurrent_activation ='sigmoid', recurrent_dropout = 0, unroll=False,use_bias=True,\n","                               kernel_regularizer= regularizers.l2(1e-6))\n","        \n","        self.MonotonicBahaAtt=MonotonicBahaAtt(self.att_units,\n","                 mode=self.score_fun,\n","                 return_aweights=False,\n","                 scaling_factor=None,\n","                 noise_std=0,\n","                 weights_initializer='he_normal',\n","                 bias_initializer='zeros',)\n","\n","        self.dense = Dense(units=self.tar_vocab_size)\n","\n","    def call(self,input_to_decoder, encoder_output, state_h,state_c, attention_weights):\n","        \n","        d_embedd = self.decod_embedd(input_to_decoder)\n","\n","        context_vector,attention_weights = self.MonotonicBahaAtt(state_h,encoder_output, attention_weights)\n","\n","        d_embedd = tf.concat([tf.expand_dims(context_vector,1), d_embedd], axis=-1)\n","\n","        d_out, d_hidden, d_cell = self.decod_LSTM(d_embedd, initial_state=[state_h, state_c])\n","\n","        output = self.dense(d_out)\n","        #print(d_out.shape)\n","\n","        return output, d_hidden, d_cell, attention_weights, context_vector\n","\n","    def get_config(self):\n","\n","        config = {'tar_vocab_size': self.tar_vocab_size,\n","                  'embedding_dim': self.embedding_dim,\n","                  'input_length': self.input_length,\n","                  'dec_units': self.dec_units,\n","                  'score_fun': self.score_fun,\n","                  'att_units': self.att_units\n","                  }\n","        \n","        base_config = super(OneStepDecoder, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","       return cls(**config)"],"id":"structural-strengthening","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"resident-expression"},"source":["### Decoder"],"id":"resident-expression"},{"cell_type":"code","metadata":{"id":"color-vertex"},"source":["class Decoder(tf.keras.Model):\n","    def __init__(self,out_vocab_size, embedding_dim, output_length, dec_units ,score_fun ,att_units):\n","        super().__init__()\n","\n","        self.out_vocab_size = out_vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.output_length = output_length\n","        self.dec_units = dec_units\n","        self.score_fun = score_fun\n","        self.att_units = att_units\n","\n","\n","    #def build(self, input_shape):\n","        self.onestep_decoder = OneStepDecoder(self.out_vocab_size, self.embedding_dim, self.output_length, self.dec_units,\n","                                              self.score_fun ,self.att_units)\n","\n","        \n","    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state, attention_weights ):\n","\n","        all_outputs = tf.TensorArray(tf.float32, size=tf.shape(input_to_decoder)[1], name='output_arrays')  #size=input_to_decoder.shape[1]\n","\n","        #shape = tf.shape(input_to_decoder)[1]\n","        #input_to_decoder.shape[1]  #range((tf.shape(input_to_decoder)[1]))\n","        #itr = tf.cast((tf.shape(input_to_decoder)[1]), dtype='int32')\n","        #itr = input_to_decoder.get_shape().as_list()[1]\n","        itr = tf.shape(input_to_decoder)[1]\n","\n","        #_,itr = tf.shape(input_to_decoder)\n","        #itr = int(itr)\n","\n","        for tstep in range(itr):    \n","\n","            output, decoder_hidden_state, decoder_cell_state, attention_weights, context_vector = self.onestep_decoder(\n","            input_to_decoder[:, tstep:tstep+1], encoder_output, decoder_hidden_state, \n","                                 decoder_cell_state, attention_weights)                      #default teacher forcing\n","\n","            all_outputs = all_outputs.write(tstep, output)\n","        \n","        all_outputs = tf.transpose(all_outputs.stack(), [1,0,2])\n","\n","        return all_outputs\n","\n","    def get_config(self):\n","        config = {'out_vocab_size': self.out_vocab_size,\n","                  'embedding_dim': self.embedding_dim,\n","                  'output_length': self.output_length,\n","                  'dec_units': self.dec_units,\n","                  'score_fun': self.score_fun,\n","                  'att_units': self.att_units}\n","\n","        base_config = super(Decoder, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","        "],"id":"color-vertex","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzqtkKJmUY8B"},"source":["## Data pipeline"],"id":"WzqtkKJmUY8B"},{"cell_type":"code","metadata":{"id":"aOzdNqscUY8O","executionInfo":{"status":"ok","timestamp":1615639497097,"user_tz":-330,"elapsed":1392,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["class Dataset:\n","    def __init__(self, data, token_perturbation, token_correct, max_len):\n","        self.encoder_inps = data['encoder_input'].values\n","        self.decoder_inps = data['decoder_input'].values\n","        self.decoder_outs = data['decoder_output'].values\n","        self.token_correct = token_correct\n","        self.token_perturbation = token_perturbation\n","        self.max_len = max_len\n","\n","    def __getitem__(self, i):\n","        self.encoder_seq = self.token_perturbation.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n","        self.decoder_inp_seq = self.token_correct.texts_to_sequences([self.decoder_inps[i]])\n","        self.decoder_out_seq = self.token_correct.texts_to_sequences([self.decoder_outs[i]])\n","\n","        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='float32', padding='post')\n","        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='float32', padding='post')\n","        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='float32', padding='post')\n","        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n","\n","    def __len__(self): # your model.fit_gen requires this function\n","        return len(self.encoder_inps)\n","\n"],"id":"aOzdNqscUY8O","execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQz85IDUUY8P","executionInfo":{"status":"ok","timestamp":1615639497098,"user_tz":-330,"elapsed":1110,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["class Dataloader(tf.keras.utils.Sequence):\n","    \n","    def __init__(self, dataset, batch_size=1):\n","        self.dataset = dataset\n","        self.batch_size = batch_size\n","        self.indexes = np.arange(len(self.dataset.encoder_inps))\n","\n","\n","    def __getitem__(self, i):\n","        start = i * self.batch_size\n","        stop = (i + 1) * self.batch_size\n","        data = []\n","        for j in range(start, stop):\n","            data.append(self.dataset[j])\n","\n","        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n","        \n","        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n","        \n","        return [batch[0],batch[1]],batch[2]\n","\n","    def __len__(self):  # your model.fit_gen requires this function\n","        return len(self.indexes) // self.batch_size\n","\n","    def on_epoch_end(self):\n","        self.indexes = np.random.permutation(self.indexes)"],"id":"CQz85IDUUY8P","execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"virtual-atlanta"},"source":["### Model"],"id":"virtual-atlanta"},{"cell_type":"code","metadata":{"id":"latest-hunter"},"source":["class attention_model(tf.keras.Model):\n","    def __init__(self,e_vocab_size, d_vocab_size, embedding_dim_e,embedding_dim_d, i_length, o_length, enc_units, \n","                 dec_units ,score_fun ,att_units):\n","        super().__init__()\n","\n","        self.e_vocab_size = e_vocab_size\n","        self.d_vocab_size = d_vocab_size\n","        self.embedding_dim_e = embedding_dim_e\n","        self.embedding_dim_d = embedding_dim_d\n","        self.i_length = i_length\n","        self.o_length = o_length\n","        self.enc_units = enc_units\n","        self.dec_units = dec_units\n","        self.score_fun = score_fun\n","        self.att_units = att_units\n","\n","    #def build(self, input_shape):\n","        self.encoder = Encoder(self.e_vocab_size,self.embedding_dim_e,self.enc_units,self.i_length)\n","        self.decoder = Decoder(self.d_vocab_size,self.embedding_dim_d,self.o_length,self.dec_units,self.score_fun,self.att_units)\n","\n","    def call(self, data, training=True):\n","        e_input,d_input = data[0], data[1]\n","        \n","        e_output, e_hidden, e_cell = self.encoder(e_input)\n","    \n","        d_hidden = e_hidden  #initial decoder state is equal to final encoder hidden state\n","        d_cell = e_cell\n","\n","        attention_weights = np.zeros((512, 16), dtype='float32')\n","        attention_weights[:, 0] = 1\n","        \n","        final= self.decoder(d_input,e_output,d_hidden,d_cell, attention_weights)\n","        return final\n","\n","    def get_config(self):\n","        \n","        config={'e_vocab_size': self.e_vocab_size,\n","                'd_vocab_size': self.d_vocab_size,\n","                'embedding_dim_e': self.embedding_dim_e,\n","                'embedding_dim_d': self.embedding_dim_d,\n","                'i_length': self.i_length,\n","                'o_length': self.o_length,\n","                'enc_units': self.enc_units,\n","                'dec_units': self.dec_units,\n","                'score_fun': self.score_fun,\n","                'att_units': self.att_units}\n","        \n","        base_config = super(attention_model, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","\n"],"id":"latest-hunter","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"successful-incentive"},"source":["## Custom Loss Function"],"id":"successful-incentive"},{"cell_type":"code","metadata":{"id":"statewide-bouquet","executionInfo":{"status":"ok","timestamp":1615639502348,"user_tz":-330,"elapsed":1003,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}}},"source":["optimizer = tf.keras.optimizers.Adam(clipnorm=1)\n","#loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"id":"statewide-bouquet","execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scenic-saying"},"source":["## Datagenerator"],"id":"scenic-saying"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"handed-hobby","executionInfo":{"status":"ok","timestamp":1615639505730,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"f791fea7-e14d-4911-d918-6b4878ed453d"},"source":["train_dataset = Dataset(train, token_perturbation, token_correct, 16)\n","validation_dataset  = Dataset(validation, token_perturbation, token_correct, 16)\n","test_dataset = Dataset(test, token_perturbation, token_correct, 16)\n","\n","train_dataloader = Dataloader(train_dataset, batch_size=512)\n","validation_dataloader = Dataloader(validation_dataset, batch_size=512)\n","test_dataloader = Dataloader(test_dataset, batch_size=512)\n","\n","print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"],"id":"handed-hobby","execution_count":25,"outputs":[{"output_type":"stream","text":["(512, 16) (512, 16) (512, 16)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gcoeFuRQipdX"},"source":["***"],"id":"gcoeFuRQipdX"},{"cell_type":"code","metadata":{"id":"08aAPsMIipGK"},"source":["pickle.dump(attention_model,open('/content/drive/MyDrive/Project_2/att.pkl','wb'))"],"id":"08aAPsMIipGK","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["att = pickle.load(open('/content/drive/MyDrive/Project_2/att.pkl','rb'))"]},{"cell_type":"markdown","metadata":{"id":"unable-snapshot"},"source":["## Training"],"id":"unable-snapshot"},{"cell_type":"code","metadata":{"id":"enhanced-location"},"source":["e_vocab_size = v_perturbation\n","d_vocab_size = v_correct\n","embedding_dim_e = 300\n","embedding_dim_d = 300\n","i_length = 16\n","o_length = 16\n","enc_units = 300\n","dec_units = 300\n","score_fun = 'parallel'\n","att_units = 300"],"id":"enhanced-location","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgX_JLBMi--y"},"source":["attention__ = att(e_vocab_size, d_vocab_size, embedding_dim_e,embedding_dim_d, i_length, o_length, enc_units, \r\n","                 dec_units ,score_fun ,att_units)"],"id":"fgX_JLBMi--y","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"higher-township"},"source":["attention__ = attention_model(e_vocab_size, d_vocab_size, embedding_dim_e,embedding_dim_d, i_length, o_length, enc_units, \n","                 dec_units ,score_fun ,att_units)"],"id":"higher-township","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"continuous-porter"},"source":["attention__.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])"],"id":"continuous-porter","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"professional-homework"},"source":["### Callbacks"],"id":"professional-homework"},{"cell_type":"code","metadata":{"id":"contemporary-mainstream"},"source":["import datetime\n","logfile2 = \"/content/drive/MyDrive/Project_2/logs/attention/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#logfile2 = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\logs\\\\attention\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tfboard = tf.keras.callbacks.TensorBoard(log_dir=logfile2, histogram_freq=1, write_graph=True)\n","\n","chkfile2 = \"/content/drive/MyDrive/Project_2/wts/attention/weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n","#chkfile2 = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\wts\\\\attention\\\\weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n","chkpt =tf.keras.callbacks.ModelCheckpoint(chkfile2, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=0, mode='min')\n","\n","es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='min')"],"id":"contemporary-mainstream","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3o7-eu2oSWb"},"source":["attention__(tf.ones(shape=(512,512,16)))"],"id":"l3o7-eu2oSWb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"potential-institution"},"source":["attention__.load_weights(\"/content/drive/MyDrive/Project_2/wts/attention/weights-21-0.2798.hdf5\")\r\n","#attention__.load_weights(\"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\wts\\\\attention\\\\mono\\\\weights-36-0.9240.hdf5\")"],"id":"potential-institution","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Du3AJM6ATfKO"},"source":["train_steps=train.shape[0]//512\r\n","valid_steps=validation.shape[0]//512\r\n","attention__.fit(train_dataloader, steps_per_epoch=train_steps, epochs=120, validation_data=validation_dataloader,\r\n","                         validation_steps=valid_steps, )"],"id":"Du3AJM6ATfKO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MhQi843yRR9"},"source":["attention__.save('/content/drive/MyDrive/Project_2/Pipeline/attention_model/')"],"id":"8MhQi843yRR9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mf_eUs-iAeyr"},"source":["## Inference"],"id":"Mf_eUs-iAeyr"},{"cell_type":"code","metadata":{"id":"AaHws-kRgE8l"},"source":["decoder_layer = attention__.layers[1]"],"id":"AaHws-kRgE8l","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHvHmULdgE8m"},"source":["onestep = decoder_layer.layers[0]"],"id":"xHvHmULdgE8m","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swqUq-kvvpdQ"},"source":["def inference_att(enc_inp):\r\n","\r\n","    enc_inp = enc_inp.replace(\",\", \"\")\r\n","    enc_inp = enc_inp.replace(\"?\", \"\")\r\n","    enc_inp = enc_inp.replace(\"!\", \"\")\r\n","    translation=\"\"\r\n","    \r\n","    e_input=[]\r\n","    for i in enc_inp.split():\r\n","        if token_perturbation.word_index.get(i) == None:\r\n","            e_input.append(0)\r\n","        else:\r\n","            e_input.append(token_perturbation.word_index.get(i))\r\n","    #e_input = [token_perturbation.word_index[i] for i in enc_inp.split(\" \")]\r\n","    e_input = pad_sequences([e_input], maxlen=16, padding='post')\r\n","    \r\n","    \r\n","    e_output, e_hidden, e_cell = attention__.layers[0](e_input,0)\r\n","    \r\n","    d_hidden = e_hidden\r\n","    d_cell = e_cell                   #final encoder state is equal to initial decoder state\r\n","    \r\n","    #initial decoder input is start\r\n","    d_input = tf.expand_dims([token_correct.word_index['<start>']],0)\r\n","\r\n","    attention_weights = np.zeros((1,16), dtype='float32')\r\n","    attention_weights[:,0] = 1\r\n","\r\n","    for word in range(16):\r\n","        \r\n","        predicted, d_hidden, d_cell, attention_weights, context_vector = onestep(d_input, e_output, d_hidden, d_cell,\r\n","                                                                                 attention_weights)        \r\n","        \r\n","        #attention_weights = tf.reshape(attention_weights, (-1,))  #creating a 1d tensor attention weights to store them\r\n","        #attention_plot[word] = attention_weights.numpy()\r\n","        \r\n","        #making predictions\r\n","        predicted_word_index = tf.argmax(predicted[0]).numpy()  #it will give the highest probability word\r\n","      \r\n","\r\n","        #stopping when reaching \"<end>\"\r\n","        if token_correct.index_word[predicted_word_index] == '<end>':\r\n","            return translation #, attention_plot\r\n","        \r\n","        #appending the result with predicted words\r\n","        translation = translation + token_correct.index_word[predicted_word_index] + ' '\r\n","        \r\n","        # the predicted ID is fed back into the model\r\n","        d_input = tf.expand_dims([predicted_word_index], 0)\r\n","        \r\n","    return translation #, attention_plot"],"id":"swqUq-kvvpdQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjO-xrHRzwCF","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1615638559273,"user_tz":-330,"elapsed":1037,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"8b4ae028-af90-4f02-d02e-483812756641"},"source":["inference_att('i have an pen')"],"id":"WjO-xrHRzwCF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'i have a pen '"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"lZpz8V-Dzv48"},"source":[],"id":"lZpz8V-Dzv48","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"HOEH3BSqAeyy","executionInfo":{"status":"ok","timestamp":1615186845045,"user_tz":-330,"elapsed":73227,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"15ba8e52-b0b4-4664-bdd6-ec4bdc378680"},"source":["sample_train = train.sample(1000)\n","result = []\n","for enc_inp,dec_inp,_ in sample_train.values:\n","    pred = inference_att(enc_inp)\n","    result.append(pred)\n","sample_train['correct_output'] = sample_train['decoder_output']\n","sample_train['predicted_output'] = result\n","sample_train = sample_train.drop(['decoder_input', 'decoder_output'], axis=1)\n","sample_train.head(10)"],"id":"HOEH3BSqAeyy","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>correct_output</th>\n","      <th>predicted_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>185429</th>\n","      <td>what an way to finish</td>\n","      <td>what a way to finish &lt;end&gt;</td>\n","      <td>what a way to finish</td>\n","    </tr>\n","    <tr>\n","      <th>180952</th>\n","      <td>you wanna have drink before we start dancing</td>\n","      <td>you wanna have a drink before we start dancing...</td>\n","      <td>you wanna have a drink before we start dancing</td>\n","    </tr>\n","    <tr>\n","      <th>100305</th>\n","      <td>imagine an important part of my life wasted be...</td>\n","      <td>imagine an important part of my life wasted be...</td>\n","      <td>imagine an important part of my life wasted be...</td>\n","    </tr>\n","    <tr>\n","      <th>171689</th>\n","      <td>all way to urbana for a one night stand</td>\n","      <td>all the way to urbana  for a one night stand &lt;...</td>\n","      <td>all the way to urbana for a one night</td>\n","    </tr>\n","    <tr>\n","      <th>31490</th>\n","      <td>can we leave out an bowl of milk mommy</td>\n","      <td>can we leave out a bowl of milk mommy &lt;end&gt;</td>\n","      <td>can we leave out a bowl of milk mommy</td>\n","    </tr>\n","    <tr>\n","      <th>63907</th>\n","      <td>i am assuming that is what made him dead one</td>\n","      <td>i am assuming that is what made him the dead o...</td>\n","      <td>i am assuming that is what made him a dead one</td>\n","    </tr>\n","    <tr>\n","      <th>143783</th>\n","      <td>what waste of time</td>\n","      <td>what a waste of time &lt;end&gt;</td>\n","      <td>what a waste of time</td>\n","    </tr>\n","    <tr>\n","      <th>158086</th>\n","      <td>that why you brought him</td>\n","      <td>is that why you brought him &lt;end&gt;</td>\n","      <td>that is why you brought him</td>\n","    </tr>\n","    <tr>\n","      <th>75998</th>\n","      <td>but i have got weak stomach</td>\n","      <td>but i have got a weak stomach &lt;end&gt;</td>\n","      <td>but i have got a weak stomach</td>\n","    </tr>\n","    <tr>\n","      <th>169482</th>\n","      <td>is david dunne their</td>\n","      <td>is david dunne there &lt;end&gt;</td>\n","      <td>is david dunne</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            encoder_input  ...                                   predicted_output\n","185429                              what an way to finish  ...                              what a way to finish \n","180952       you wanna have drink before we start dancing  ...    you wanna have a drink before we start dancing \n","100305  imagine an important part of my life wasted be...  ...  imagine an important part of my life wasted be...\n","171689            all way to urbana for a one night stand  ...             all the way to urbana for a one night \n","31490              can we leave out an bowl of milk mommy  ...             can we leave out a bowl of milk mommy \n","63907        i am assuming that is what made him dead one  ...    i am assuming that is what made him a dead one \n","143783                                 what waste of time  ...                              what a waste of time \n","158086                           that why you brought him  ...                       that is why you brought him \n","75998                         but i have got weak stomach  ...                     but i have got a weak stomach \n","169482                               is david dunne their  ...                                    is david dunne \n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"KhjGWqs1zz9K","executionInfo":{"status":"ok","timestamp":1615391175353,"user_tz":-330,"elapsed":118874,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"65cab3e0-cbcc-4ff8-b849-8a45a9ff8406"},"source":["sample_validation = validation.sample(1000)\r\n","result = []\r\n","for enc_inp,dec_inp,_ in sample_validation.values:\r\n","    pred = inference_att(enc_inp)\r\n","    result.append(pred)\r\n","sample_validation['correct_output'] = sample_validation['decoder_output']\r\n","sample_validation['predicted_output'] = result\r\n","sample_validation = sample_validation.drop(['decoder_input', 'decoder_output'], axis=1)\r\n","sample_validation.head(10)"],"id":"KhjGWqs1zz9K","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>correct_output</th>\n","      <th>predicted_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>193097</th>\n","      <td>every so often they imagine story good enough ...</td>\n","      <td>every so often they imagine a story good enoug...</td>\n","      <td>every so often they imagine the story good eno...</td>\n","    </tr>\n","    <tr>\n","      <th>201723</th>\n","      <td>out of mouths of babes louis</td>\n","      <td>out of the mouths of babes louis &lt;end&gt;</td>\n","      <td>out of the trees of crackers</td>\n","    </tr>\n","    <tr>\n","      <th>155259</th>\n","      <td>whatever happened was accident</td>\n","      <td>whatever happened was an accident &lt;end&gt;</td>\n","      <td>whatever was happened was an accident</td>\n","    </tr>\n","    <tr>\n","      <th>123156</th>\n","      <td>christ i did not say you were out of game</td>\n","      <td>christ i did not say you were out of the game ...</td>\n","      <td>christ i did not say you were out of the game</td>\n","    </tr>\n","    <tr>\n","      <th>184780</th>\n","      <td>that your game</td>\n","      <td>is that your game &lt;end&gt;</td>\n","      <td>that is your game</td>\n","    </tr>\n","    <tr>\n","      <th>12080</th>\n","      <td>in an manner of speaking</td>\n","      <td>in a manner of speaking &lt;end&gt;</td>\n","      <td>in a manner of speaking</td>\n","    </tr>\n","    <tr>\n","      <th>190064</th>\n","      <td>well it certainly is coincidence</td>\n","      <td>well it certainly is a coincidence &lt;end&gt;</td>\n","      <td>well it is certainly a coincidence</td>\n","    </tr>\n","    <tr>\n","      <th>159590</th>\n","      <td>i am sorry it was an accident</td>\n","      <td>i am sorry it was an accident &lt;end&gt;</td>\n","      <td>i am sorry it was an accident</td>\n","    </tr>\n","    <tr>\n","      <th>484</th>\n","      <td>dursleys ducked but harry leapt into air tryi...</td>\n","      <td>the dursleys ducked but harry leapt into the a...</td>\n","      <td>the dursleys he wins harry into the supplies d...</td>\n","    </tr>\n","    <tr>\n","      <th>103312</th>\n","      <td>the girl is name doreen</td>\n","      <td>the girl is name was doreen &lt;end&gt;</td>\n","      <td>the girl is name was</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            encoder_input  ...                                   predicted_output\n","193097  every so often they imagine story good enough ...  ...  every so often they imagine the story good eno...\n","201723                       out of mouths of babes louis  ...                      out of the trees of crackers \n","155259                     whatever happened was accident  ...             whatever was happened was an accident \n","123156          christ i did not say you were out of game  ...     christ i did not say you were out of the game \n","184780                                     that your game  ...                                 that is your game \n","12080                            in an manner of speaking  ...                           in a manner of speaking \n","190064                   well it certainly is coincidence  ...                well it is certainly a coincidence \n","159590                      i am sorry it was an accident  ...                     i am sorry it was an accident \n","484      dursleys ducked but harry leapt into air tryi...  ...  the dursleys he wins harry into the supplies d...\n","103312                            the girl is name doreen  ...                              the girl is name was \n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"mzfE202Xz2sm"},"source":["***"],"id":"mzfE202Xz2sm"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"tNlOPs4S0XmE","executionInfo":{"status":"ok","timestamp":1615391313644,"user_tz":-330,"elapsed":118140,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"cc82ba7a-022d-46e0-bbb5-6a3ef822136b"},"source":["sample_test = test.sample(1000)\r\n","result = []\r\n","for enc_inp,dec_inp,_ in sample_test.values:\r\n","    pred = inference_att(enc_inp)\r\n","    result.append(pred)\r\n","sample_test['correct_output'] = sample_test['decoder_output']\r\n","sample_test['predicted_output'] = result\r\n","sample_test = sample_test.drop(['decoder_input', 'decoder_output'], axis=1)\r\n","sample_test.head(10)"],"id":"tNlOPs4S0XmE","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>encoder_input</th>\n","      <th>correct_output</th>\n","      <th>predicted_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>164628</th>\n","      <td>you look like you were very sensitive child</td>\n","      <td>you look like you were a very sensitive child ...</td>\n","      <td>you look like you were a very sensitive child</td>\n","    </tr>\n","    <tr>\n","      <th>194417</th>\n","      <td>and main reason ellie is that way is because y...</td>\n","      <td>and the main reason ellie is that way is becau...</td>\n","      <td>the reason ellie is that goes the all cause yo...</td>\n","    </tr>\n","    <tr>\n","      <th>148272</th>\n","      <td>so what you doing later</td>\n","      <td>so what are you doing later  &lt;end&gt;</td>\n","      <td>so what are you doing later</td>\n","    </tr>\n","    <tr>\n","      <th>42984</th>\n","      <td>you never played violin</td>\n","      <td>you never played a violin &lt;end&gt;</td>\n","      <td>you never played the violin</td>\n","    </tr>\n","    <tr>\n","      <th>186169</th>\n","      <td>fifty will get you hundred that he did not do it</td>\n","      <td>fifty will get you a hundred that he did not d...</td>\n","      <td>fifty will get you a hundred that he did not d...</td>\n","    </tr>\n","    <tr>\n","      <th>29290</th>\n","      <td>i mean yeah i would say this an all right shirt</td>\n","      <td>i mean yeah i would say this was an all right ...</td>\n","      <td>i mean yeah i would say this is all right an s...</td>\n","    </tr>\n","    <tr>\n","      <th>107313</th>\n","      <td>they said hello</td>\n","      <td>he said hello &lt;end&gt;</td>\n","      <td>he said hello</td>\n","    </tr>\n","    <tr>\n","      <th>11459</th>\n","      <td>they fucked around with it and torah melted th...</td>\n","      <td>they fucked around with it and the torah melte...</td>\n","      <td>they fucked around with it and a tarp their fa...</td>\n","    </tr>\n","    <tr>\n","      <th>198668</th>\n","      <td>you looking for catherine not me</td>\n","      <td>you are looking for catherine not me &lt;end&gt;</td>\n","      <td>you are looking for catherine not me</td>\n","    </tr>\n","    <tr>\n","      <th>100080</th>\n","      <td>we shoulda been their to back you up</td>\n","      <td>we shoulda been there to back you up &lt;end&gt;</td>\n","      <td>we shoulda been there to back you</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            encoder_input  ...                                   predicted_output\n","164628        you look like you were very sensitive child  ...     you look like you were a very sensitive child \n","194417  and main reason ellie is that way is because y...  ...  the reason ellie is that goes the all cause yo...\n","148272                           so what you doing later   ...                       so what are you doing later \n","42984                             you never played violin  ...                       you never played the violin \n","186169   fifty will get you hundred that he did not do it  ...  fifty will get you a hundred that he did not d...\n","29290     i mean yeah i would say this an all right shirt  ...  i mean yeah i would say this is all right an s...\n","107313                                    they said hello  ...                                     he said hello \n","11459   they fucked around with it and torah melted th...  ...  they fucked around with it and a tarp their fa...\n","198668                   you looking for catherine not me  ...              you are looking for catherine not me \n","100080               we shoulda been their to back you up  ...                 we shoulda been there to back you \n","\n","[10 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"-QN46YYnAeyz"},"source":["## BLEU Score"],"id":"-QN46YYnAeyz"},{"cell_type":"code","metadata":{"id":"Cgd0YKoQAey0"},"source":["import nltk.translate.bleu_score as bleu"],"id":"Cgd0YKoQAey0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-YGFnCKLTj2","executionInfo":{"status":"ok","timestamp":1615187440217,"user_tz":-330,"elapsed":1352,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"0b6350de-3c68-4aab-d300-da5e052436a8"},"source":["train_bleu = []\r\n","for enc_inp,correct,pred in sample_train.values:\r\n","    \r\n","    correct = correct.split()[:-1]    #removing end\r\n","    pred = pred.split()\r\n","    \r\n","    if len(correct) == len(pred):\r\n","        train_bleu.append(bleu.sentence_bleu([correct],pred))\r\n","        \r\n","print(\"BLEU Score of train dataset is\",sum(train_bleu)/len(train_bleu))"],"id":"z-YGFnCKLTj2","execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU Score of train dataset is 0.978379354270412\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"IFlzk2dBMM3o"},"source":["<font size=4 color='red'> BLEU Score of train dataset is 0.9948193553886482"],"id":"IFlzk2dBMM3o"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQJU0cHVAey0","executionInfo":{"status":"ok","timestamp":1615391313646,"user_tz":-330,"elapsed":107041,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"922db09f-5abb-4ff5-d55d-083723416356"},"source":["validation_bleu = []\n","for enc_inp,correct,pred in sample_validation.values:\n","    \n","    correct = correct.split()[:-1]    #removing end\n","    pred = pred.split()\n","\n","    if len(correct) == len(pred):\n","        validation_bleu.append(bleu.sentence_bleu([correct],pred))\n","        \n","print(\"BLEU Score of validation dataset is\",sum(validation_bleu)/len(validation_bleu))"],"id":"BQJU0cHVAey0","execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU Score of validation dataset is 0.8905224236711871\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Cngu74x-MRGP"},"source":["<font size=4 color='red'> BLEU Score of validation dataset is 0.8905224236711871\r\n","\r\n"],"id":"Cngu74x-MRGP"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpxPd1kP0dxC","executionInfo":{"status":"ok","timestamp":1615391313647,"user_tz":-330,"elapsed":92102,"user":{"displayName":"Tiku Work","photoUrl":"","userId":"02665959882814034002"}},"outputId":"4e2cfdea-f9be-41b8-a1a6-e01a34a99b94"},"source":["test_bleu = []\r\n","for enc_inp,correct,pred in sample_test.values:\r\n","    \r\n","    correct = correct.split()[:-1]    #removing end\r\n","    pred = pred.split()\r\n","\r\n","    if len(correct) == len(pred):\r\n","        test_bleu.append(bleu.sentence_bleu([correct],pred))\r\n","        \r\n","print(\"BLEU Score of test dataset is\",sum(test_bleu)/len(test_bleu))"],"id":"rpxPd1kP0dxC","execution_count":null,"outputs":[{"output_type":"stream","text":["BLEU Score of test dataset is 0.899345597466305\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 4-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 3-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"QBz6sUyzMSXm"},"source":["<font size=4 color='red'> BLEU Score of test dataset is 0.899345597466305"],"id":"QBz6sUyzMSXm"},{"cell_type":"markdown","metadata":{"id":"9JhAW8CY6voL"},"source":["## Error Analysis"],"id":"9JhAW8CY6voL"},{"cell_type":"markdown","metadata":{"id":"DtbMDody6K7z"},"source":["<font size=4, color='red'> 1. Many words and sentence in our corpus are not normal, meaning they are in  movie language so embedding matrix returns 0 for those words as they are  incorrect in proper english.  [This is a major reason]</font>  \r\n","__Ex.__  \r\n","ui amu the director uiu make the casting decis.  \r\n","we shoulda been their to back you up\t\r\n","yer doing whacha\r\n","i enclose a representation of mbwun  \r\n","wut up\r\n","***"],"id":"DtbMDody6K7z"},{"cell_type":"markdown","metadata":{"id":"yAJZDkgG4s2m"},"source":["<font size=4, color=\"red\">2. confusion between whether to add \"a\" , \"an\"  or replace them with \"the\" </font>  \r\n","__Ex.__  \r\n","incorrect-->you never played violin  \r\n","correct-->you never played a violin  \r\n","predicted--> you never played the violin  \r\n","\r\n","**_Note: Happens in a few cases only_**\r\n","***"],"id":"yAJZDkgG4s2m"},{"cell_type":"markdown","metadata":{"id":"bl-dY7HY-cLJ"},"source":["<font size=4, color='red'>3. Gets confused while dealing with names because there are not enough sentence which contains same name. </font>  \r\n","__Ex.__  \r\n","incorrect-->japan four thousand miles away  \r\n","correct-->japan is four thousand miles away  \r\n","pred-->steveston is eight thousand miles \r\n","\r\n","***"],"id":"bl-dY7HY-cLJ"},{"cell_type":"markdown","metadata":{"id":"feW_2rLZ3uPG"},"source":["<font size=4, color='red'> 4. questions starting with 'is' 'are' are converted to affirmative sentence because it was trained on \"is\" \"are\" as perturbations.  </font>  \r\n","__Ex.__  \r\n","incorrect-->that your game  \r\n","correct-->is that your game  \r\n","predicted-->that is your game  \r\n","\r\n","**_Note: Happens in a few cases only_**"],"id":"feW_2rLZ3uPG"}]}