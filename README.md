# DeepTextCorrector_MonotonicAttention

[![HitCount](http://hits.dwyl.com/rishang007/DeepTextCorrector_MonotonicAttention.svg)](http://hits.dwyl.com/rishang007/DeepTextCorrector_MonotonicAttention)

## Introduction  

Sequence-to-Sequence models are proving to be one of the best applications of Artificial Intelligence in the field of Natural Language Processing. Attention mechanism has improved the seq2seq models a lot by a simple concept of mimicking the human way of understanding a sentence.  

### In this blog, I have build a machine learning model which corrects basic grammatical mistakes in a sentence using Monotonic Attention.  This was packaged and made live on AWS server with a basic UI on a webpage


Some of the perturbations addressed by my model are-  

Correcting the mistake of using determiners (a, an, the).  
Removing subordinate clause - "that".  
Replacing word modal ('may' → 'would')  
Removing verb forms ('is', 'are', 'was', 'were')  
Replacing "than" with "then" and vice-versa.  
Replacing "he/she" with "they".  
