# DeepTextCorrector_MonotonicAttention

## Introduction  

Sequence-to-Sequence models are proving to be one of the best applications of Artificial Intelligence in the field of Natural Language Processing. Attention mechanism has improved the seq2seq models a lot by a simple concept of mimicking the human way of understanding a sentence.  

### In this blog, I have build a machine learning model which corrects basic grammatical mistakes in a sentence using Monotonic Attention.  


Some of the perturbations addressed by my model are-  

Correcting the mistake of using determiners (a, an, the).
Removing subordinate clause - "that".
Replacing word modal ('may' → 'would')
Removing verb forms ('is', 'are', 'was', 'were')
Replacing "than" with "then" and vice-versa.
Replacing "he/she" with "they".
